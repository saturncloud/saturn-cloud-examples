{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Tips and Tricks: Rolling Averages in Dask\n",
    "\n",
    "How can a user take advantage of Dask parallelism while calculating rolling averages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"I need to calculate a rolling average of a numerical column, in time series data. In pandas, I can do this with rolling(x).mean() with sorted values, but what do I do in Dask, with distributed data?\"\n",
    "\n",
    "\n",
    "Great question! Time series data often poses unique challenges with distributed data and parallelization, but Dask can do it. Here's what we need to do.\n",
    "\n",
    "* Sort by index within AND across partitions\n",
    "* Know when to compute (convert to Pandas DF) or persist (process computations on cluster)\n",
    "* Fill in gaps in the date index, if any. This example uses data that has some days missing - if you need to ensure every day is represented, but have gaps in your dataset, you'll need to take steps to fill in the index from a time series (shown below).\n",
    "* Run calculations, with attention to our need to cross partitions correctly.\n",
    "\n",
    "\n",
    "This example will walk you through these specific points, and demonstrate how it's done. We'll use New York City taxi trip data, and get the 30-day rolling average of base fare prices, for our example. Also, in order to really show how this can improve your life, we have chosen data too large to be held in memory at one time in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up A Cluster\n",
    "\n",
    "This is going to employ a three worker CPU machine cluster, so we can handle some large data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-19 19:08:52] INFO - dask-saturn | Cluster is ready\n"
     ]
    }
   ],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    scheduler_size='medium',\n",
    "    worker_size='xlarge',\n",
    "    n_workers=3,\n",
    "    nthreads=4,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://d-steph-ml-workshop-443217fa1282437da4d84713212eacde.main-namespace:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-steph-ml-workshop-443217fa1282437da4d84713212eacde.internal.saturnenterprise.io' target='_blank'>https://d-steph-ml-workshop-443217fa1282437da4d84713212eacde.internal.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>94.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.0.11.197:8786' processes=3 threads=12, memory=94.50 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import wait\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Load Large Dataset \n",
    "\n",
    "NYC taxi data is a good use case, because it is too large to hold in pandas memory. That really shows us what Dask can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nyc-tlc/trip data/yellow_tripdata_2019-01.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-02.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-03.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-04.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-05.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-06.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-07.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-08.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-09.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-10.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-11.csv',\n",
       " 'nyc-tlc/trip data/yellow_tripdata_2019-12.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "files_2019 = 's3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv'\n",
    "s3.glob(files_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.4 ms, sys: 21.4 ms, total: 54.8 ms\n",
      "Wall time: 514 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "taxi = dd.read_csv(\n",
    "    files_2019,\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "    storage_options={'anon': True},\n",
    "    assume_missing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have our Dask Dataframe ready to go.\n",
    "\n",
    "### Dataset Size\n",
    "\n",
    "That's a lot of rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84399019"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.shape[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Shape Data\n",
    "\n",
    "Create a general date field, and ensure that all data across and within partitions is sorted by it. This index management and mapping is the most time-intensive part of this job.\n",
    "\n",
    "Format the date field, so it can be an integer index for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 ms, sys: 239 µs, total: 14 ms\n",
      "Wall time: 12.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi['date'] = taxi.tpep_pickup_datetime.dt.date\n",
    "taxi['datenum'] = taxi.date.astype('datetime64[ns]').astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the integer date column as our index. Note I am using `map_partitions` here which is how we ensure that the sorting by the index is carried out both **within** and **across** partitions. This is important! Otherwise, your partitions might not be aligned date-wise and then your rolling averages will be inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 40.1 ms, total: 1.14 s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi = taxi.set_index('datenum').map_partitions(lambda taxi: taxi.sort_index()).persist()\n",
    "_ = wait(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### A Note On Persist and Compute\n",
    "\n",
    "Lots of new users of Dask find the `.persist()` and `.compute()` processes confusing. This is understandable! But the answer is not as hard as you might think.\n",
    "\n",
    "First, remember we have several machines working for us right now. We have our Jupyter instance right here running on one, and then our cluster of three worker machines also.\n",
    "\n",
    "If we use `.compute()`, we are asking Dask to take all the computations and adjustments to the data that we have queued up, and run them, and bring it all to the surface here, in Jupyter. That means if it *was* distributed we want to convert it into a local object here and now. If it's a Dask Dataframe, when we call `.compute()`, we're saying \"Run the transformations we've queued, and convert this into a pandas dataframe immediately.\". If our data is too big to be held in local pandas memory, this can be a disaster! But if it is small, then we might be fine.\n",
    "\n",
    "If we use `.persist()`, we are asking Dask to take all the computations and adjustments to the data that we have queued up, and run them, but then the object is going to remain distributed and will live on the cluster, not on the Jupyter instance. So when we do this with a Dask Dataframe, we are telling our cluster \"Run the transformations we've queued, and leave this as a distributed Dask Dataframe.\"\n",
    "\n",
    "So, if you want to process all the delayed tasks you've applied to a Dask object, either of these methods will do it. The difference is where your object will live at the end.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Calculations\n",
    "\n",
    "Back to work! Date (as integer) is now our index, which will speed up other processes. Our data is sorted by this index within and across partitions.\n",
    "\n",
    "> Note: `fare_amount` is the column we're going to work on, so here we'll average the fare by date, returning a Series that is the average fare per date. This means our end result is going to be the rolling average of the average daily fare - this may not be what you want to do in a real business case, but for this situation working with one value per date makes the computations easier to explain. We could change our grain to hour or minute, and aggregate that way, or not aggregate at all and fill in all the intervening time periods.\n",
    "\n",
    "\n",
    "Calculate the daily mean base fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 89.4 ms, sys: 0 ns, total: 89.4 ms\n",
      "Wall time: 683 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fares = taxi.groupby(by='datenum').fare_amount.mean().persist()\n",
    "_ = wait(fares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 ms, sys: 3.51 ms, total: 7.43 ms\n",
      "Wall time: 30.4 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_avg_fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1595462400000000000</th>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595548800000000000</th>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595635200000000000</th>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595721600000000000</th>\n",
       "      <td>2020-07-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595808000000000000</th>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595894400000000000</th>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595980800000000000</th>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596067200000000000</th>\n",
       "      <td>2020-07-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596153600000000000</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596240000000000000</th>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date  daily_avg_fare_amount\n",
       "datenum                                              \n",
       "1595462400000000000 2020-07-23                    NaN\n",
       "1595548800000000000 2020-07-24                    NaN\n",
       "1595635200000000000 2020-07-25                    9.0\n",
       "1595721600000000000 2020-07-26                    NaN\n",
       "1595808000000000000 2020-07-27                    NaN\n",
       "1595894400000000000 2020-07-28                    NaN\n",
       "1595980800000000000 2020-07-29                    NaN\n",
       "1596067200000000000 2020-07-30                    NaN\n",
       "1596153600000000000 2020-07-31                    NaN\n",
       "1596240000000000000 2020-08-01                    NaN"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fares.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our taxi dataset has some unlikely extreme dates on the outer edges, so I'm going to filter by date just to make sure we have reliable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t1 = datetime.datetime(2019, 1, 1,0,0,0)\n",
    "t2 = np.datetime64(t1, 'ns')\n",
    "t3 = t2.astype(int)\n",
    "\n",
    "t1 = datetime.datetime(2020, 8, 1,0,0,0)\n",
    "t2 = np.datetime64(t1, 'ns')\n",
    "t4 = t2.astype(int)\n",
    "\n",
    "taxi = taxi.loc[t3:t4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Fill Missing Dates in Index\n",
    "\n",
    "If you don't have gaps in your date field, then you can ignore this entirely! But for many time series use cases, you'll have missing time points, and want to fill those in before calculating a rolling aggregation. The way we'll do that here is to create a brand new complete index, using pandas functions, and then we'll merge that with our starting dataset.\n",
    "\n",
    "\n",
    "Create the index..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.Series(pd.date_range('01-01-2019', '08-01-2020'), name='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make it a Dask Dataframe, so it can be compatible with our existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dd = dd.from_pandas(idx, npartitions=1).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the date field that we've made just like we formatted our date field on the taxi data, so that we have integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dd['datenum'] = idx_dd['date'].astype('datetime64[ns]').astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then set the new `datenum` field as index on our Dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dd = idx_dd.set_index('datenum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! So here's our index for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546387200000000000</th>\n",
       "      <td>2019-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546473600000000000</th>\n",
       "      <td>2019-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546560000000000000</th>\n",
       "      <td>2019-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546646400000000000</th>\n",
       "      <td>2019-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date\n",
       "datenum                       \n",
       "1546300800000000000 2019-01-01\n",
       "1546387200000000000 2019-01-02\n",
       "1546473600000000000 2019-01-03\n",
       "1546560000000000000 2019-01-04\n",
       "1546646400000000000 2019-01-05"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply New Index\n",
    "\n",
    "So let's put it all together! We'll take our fares object, the daily average we calculated above, make it a Dask Dataframe, then merge it with the index Dask Dataframe. Remember, we are merging on index because that makes these tasks much, much faster with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fares = idx_dd.merge(fares.to_frame(), left_index= True, right_index = True, how=\"left\")\n",
    "fares = fares.rename(columns={\"fare_amount\": \"daily_avg_fare_amount\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have it- every day in the desired range is present, with the human-readable date as well as the average fare for the day. Some days have no fares, and we can see that (NaN is shown for some of the tail rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_avg_fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546387200000000000</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>13.356810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546473600000000000</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>12.564294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546560000000000000</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>12.351668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546646400000000000</th>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>11.422265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546732800000000000</th>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>12.367822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546819200000000000</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>12.437283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546905600000000000</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>12.257762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546992000000000000</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>12.239540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547078400000000000</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>12.319000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date  daily_avg_fare_amount\n",
       "datenum                                              \n",
       "1546300800000000000 2019-01-01              13.651428\n",
       "1546387200000000000 2019-01-02              13.356810\n",
       "1546473600000000000 2019-01-03              12.564294\n",
       "1546560000000000000 2019-01-04              12.351668\n",
       "1546646400000000000 2019-01-05              11.422265\n",
       "1546732800000000000 2019-01-06              12.367822\n",
       "1546819200000000000 2019-01-07              12.437283\n",
       "1546905600000000000 2019-01-08              12.257762\n",
       "1546992000000000000 2019-01-09              12.239540\n",
       "1547078400000000000 2019-01-10              12.319000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fares.tail(20)\n",
    "fares.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Desired Feature\n",
    "\n",
    "Now, we are ready to do our rolling average of the new `fare_amount` field. This is when understanding our Dask Dataframe is important. \n",
    "\n",
    "As a Dask Dataframe, our data is being stored under the surface in multiple pandas dataframes. This means we need to expect that our data is stored in different chunks, and because this next computation is order-specific/stateful, we need to get those chunks synchronized. We already made sure our dates were sorted correctly, now we need to make sure our rolling averages cross the partition breaks correctly.\n",
    "\n",
    "This next line will take care of that for us! \n",
    "* `map_overlap`: \"Map a function over blocks of arrays with some overlap\" [says the docs](https://docs.dask.org/en/latest/array-overlap.html). Our chunks are time based (our index is date!), so this will let us make sure that rolling averages that cross the break between chunks will be lined up right. We'll use a 30 day overlap to get it.\n",
    "* `rolling().mean()`: This is just what you think it is- once the data is lined up tidily with `map_overlap`, we can calculate the rolling mean at 30 days. The `min_periods` argument is important! This is how it knows that you will allow any 30 day segment with at least one non-NaN value to calculate. This is the equivalent of ignoring NaNs.\n",
    "* `persist()`: All this is doing is making the computations cache on the cluster, so that the operations run when we ask them. (Remember the note above!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 ms, sys: 239 µs, total: 11 ms\n",
      "Wall time: 10.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rolling_fares_30d = fares.map_overlap(lambda fares: fares.rolling(30, min_periods=1).mean(), 30, 0).persist() \n",
    "rolling_fares_30d = rolling_fares_30d.rename(columns={\"daily_avg_fare_amount\": \"roll_avg_fare_amount\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 8.48 ms, total: 10.4 ms\n",
      "Wall time: 42 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roll_avg_fare_amount</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_avg_fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546387200000000000</th>\n",
       "      <td>13.504119</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>13.356810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546473600000000000</th>\n",
       "      <td>13.190844</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>12.564294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546560000000000000</th>\n",
       "      <td>12.981050</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>12.351668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546646400000000000</th>\n",
       "      <td>12.669293</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>11.422265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546732800000000000</th>\n",
       "      <td>12.619048</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>12.367822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546819200000000000</th>\n",
       "      <td>12.593082</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>12.437283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546905600000000000</th>\n",
       "      <td>12.551167</td>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>12.257762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546992000000000000</th>\n",
       "      <td>12.516541</td>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>12.239540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547078400000000000</th>\n",
       "      <td>12.496787</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>12.319000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     roll_avg_fare_amount       date  daily_avg_fare_amount\n",
       "datenum                                                                    \n",
       "1546300800000000000             13.651428 2019-01-01              13.651428\n",
       "1546387200000000000             13.504119 2019-01-02              13.356810\n",
       "1546473600000000000             13.190844 2019-01-03              12.564294\n",
       "1546560000000000000             12.981050 2019-01-04              12.351668\n",
       "1546646400000000000             12.669293 2019-01-05              11.422265\n",
       "1546732800000000000             12.619048 2019-01-06              12.367822\n",
       "1546819200000000000             12.593082 2019-01-07              12.437283\n",
       "1546905600000000000             12.551167 2019-01-08              12.257762\n",
       "1546992000000000000             12.516541 2019-01-09              12.239540\n",
       "1547078400000000000             12.496787 2019-01-10              12.319000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rolling_fares2 = dd.concat([rolling_fares_30d,fares], axis = 1)\n",
    "rolling_fares2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our math, to make sure we're getting what we expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.190843999999998"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(13.651428 + 13.356810 + 12.564294)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot check in the middle of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roll_avg_fare_amount</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_avg_fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1571788800000000000</th>\n",
       "      <td>13.708770</td>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>13.985914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571875200000000000</th>\n",
       "      <td>13.705452</td>\n",
       "      <td>2019-10-24</td>\n",
       "      <td>14.315537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571961600000000000</th>\n",
       "      <td>13.678487</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>13.937396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572048000000000000</th>\n",
       "      <td>13.629068</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>13.051430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572134400000000000</th>\n",
       "      <td>13.613382</td>\n",
       "      <td>2019-10-27</td>\n",
       "      <td>13.776347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572220800000000000</th>\n",
       "      <td>13.643104</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>13.746779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572307200000000000</th>\n",
       "      <td>13.638153</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>13.426783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572393600000000000</th>\n",
       "      <td>13.642512</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>13.396489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572480000000000000</th>\n",
       "      <td>13.650316</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>13.313771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572566400000000000</th>\n",
       "      <td>13.639814</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>13.432414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     roll_avg_fare_amount       date  daily_avg_fare_amount\n",
       "datenum                                                                    \n",
       "1571788800000000000             13.708770 2019-10-23              13.985914\n",
       "1571875200000000000             13.705452 2019-10-24              14.315537\n",
       "1571961600000000000             13.678487 2019-10-25              13.937396\n",
       "1572048000000000000             13.629068 2019-10-26              13.051430\n",
       "1572134400000000000             13.613382 2019-10-27              13.776347\n",
       "1572220800000000000             13.643104 2019-10-28              13.746779\n",
       "1572307200000000000             13.638153 2019-10-29              13.426783\n",
       "1572393600000000000             13.642512 2019-10-30              13.396489\n",
       "1572480000000000000             13.650316 2019-10-31              13.313771\n",
       "1572566400000000000             13.639814 2019-11-01              13.432414"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_fares2.loc[1571788800000000000:1572566400000000000].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Feature to Original Dataset\n",
    "\n",
    "Convert the Dask Series to a Dask Dataframe, and then merge on the shared indices (\"date\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we sure the indices are comparable? We can check very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Index Structure:\n",
       "npartitions=1\n",
       "1546300800000000000    int64\n",
       "1596240000000000000      ...\n",
       "Name: datenum, dtype: int64\n",
       "Dask Name: concat-indexed, 20 tasks"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_fares2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Index Structure:\n",
       "npartitions=126\n",
       "1546300800000000000    int64\n",
       "1546387200000000000      ...\n",
       "                       ...  \n",
       "1577577600000000000      ...\n",
       "1596240000000000000      ...\n",
       "Name: datenum, dtype: int64\n",
       "Dask Name: loc, 379 tasks"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, time to merge!\n",
    "\n",
    "The merge itself is very fast here because it is lazily evaluated.\n",
    "This creates our new dataset, including all dates and with averages calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_new = taxi.join(rolling_fares2, how='outer', lsuffix = \"_day\", rsuffix= \"_rolled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(taxi_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84398118"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(taxi_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And with that, our dataset is ready! \n",
    "* All our original fields (not all shown here, for ease of reading)\n",
    "* Daily average fare\n",
    "* 30 day rolling average of fare, if at least one fare found in the last 30 days\n",
    "\n",
    "**Remember**, this has been appended back to the original object, which is too large to hold in memory, so we should not `.compute()` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>date_day</th>\n",
       "      <th>roll_avg_fare_amount</th>\n",
       "      <th>date_rolled</th>\n",
       "      <th>daily_avg_fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 16:02:52</td>\n",
       "      <td>2019-01-01 16:10:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 16:55:02</td>\n",
       "      <td>2019-01-01 17:12:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.5</td>\n",
       "      <td>14.30</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 16:12:37</td>\n",
       "      <td>2019-01-01 16:21:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 16:40:23</td>\n",
       "      <td>2019-01-01 17:04:34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>5.25</td>\n",
       "      <td>25.5</td>\n",
       "      <td>31.55</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 16:13:35</td>\n",
       "      <td>2019-01-01 16:45:16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "      <td>58.56</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 16:47:03</td>\n",
       "      <td>2019-01-01 17:10:39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.07</td>\n",
       "      <td>4.36</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.16</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 16:35:30</td>\n",
       "      <td>2019-01-01 16:59:23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.90</td>\n",
       "      <td>9.30</td>\n",
       "      <td>40.0</td>\n",
       "      <td>55.86</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 16:02:28</td>\n",
       "      <td>2019-01-01 16:17:32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.46</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.76</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 16:54:30</td>\n",
       "      <td>2019-01-01 17:05:45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.70</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.00</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546300800000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 16:28:04</td>\n",
       "      <td>2019-01-01 16:51:20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.80</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>13.651428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     VendorID tpep_pickup_datetime tpep_dropoff_datetime  \\\n",
       "datenum                                                                    \n",
       "1546300800000000000       1.0  2019-01-01 16:02:52   2019-01-01 16:10:35   \n",
       "1546300800000000000       2.0  2019-01-01 16:55:02   2019-01-01 17:12:19   \n",
       "1546300800000000000       1.0  2019-01-01 16:12:37   2019-01-01 16:21:12   \n",
       "1546300800000000000       1.0  2019-01-01 16:40:23   2019-01-01 17:04:34   \n",
       "1546300800000000000       2.0  2019-01-01 16:13:35   2019-01-01 16:45:16   \n",
       "1546300800000000000       2.0  2019-01-01 16:47:03   2019-01-01 17:10:39   \n",
       "1546300800000000000       1.0  2019-01-01 16:35:30   2019-01-01 16:59:23   \n",
       "1546300800000000000       2.0  2019-01-01 16:02:28   2019-01-01 16:17:32   \n",
       "1546300800000000000       1.0  2019-01-01 16:54:30   2019-01-01 17:05:45   \n",
       "1546300800000000000       1.0  2019-01-01 16:28:04   2019-01-01 16:51:20   \n",
       "\n",
       "                     passenger_count  trip_distance  tip_amount  fare_amount  \\\n",
       "datenum                                                                        \n",
       "1546300800000000000              1.0           1.20        0.00          7.0   \n",
       "1546300800000000000              1.0           2.69        0.00         13.5   \n",
       "1546300800000000000              1.0           1.50        2.00          8.0   \n",
       "1546300800000000000              1.0           8.40        5.25         25.5   \n",
       "1546300800000000000              1.0          16.97        0.00         52.0   \n",
       "1546300800000000000              1.0           6.07        4.36         21.0   \n",
       "1546300800000000000              2.0          14.90        9.30         40.0   \n",
       "1546300800000000000              2.0           1.04        2.46         10.5   \n",
       "1546300800000000000              3.0           2.10        2.70         10.5   \n",
       "1546300800000000000              3.0           6.00        0.00         22.0   \n",
       "\n",
       "                     total_amount    date_day  roll_avg_fare_amount  \\\n",
       "datenum                                                               \n",
       "1546300800000000000          7.80  2019-01-01             13.651428   \n",
       "1546300800000000000         14.30  2019-01-01             13.651428   \n",
       "1546300800000000000         10.80  2019-01-01             13.651428   \n",
       "1546300800000000000         31.55  2019-01-01             13.651428   \n",
       "1546300800000000000         58.56  2019-01-01             13.651428   \n",
       "1546300800000000000         26.16  2019-01-01             13.651428   \n",
       "1546300800000000000         55.86  2019-01-01             13.651428   \n",
       "1546300800000000000         14.76  2019-01-01             13.651428   \n",
       "1546300800000000000         14.00  2019-01-01             13.651428   \n",
       "1546300800000000000         22.80  2019-01-01             13.651428   \n",
       "\n",
       "                    date_rolled  daily_avg_fare_amount  \n",
       "datenum                                                 \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  \n",
       "1546300800000000000  2019-01-01              13.651428  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_new[['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime','passenger_count','trip_distance','tip_amount',\n",
    "          'fare_amount', 'total_amount', 'date_day', 'roll_avg_fare_amount','date_rolled','daily_avg_fare_amount']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>date_day</th>\n",
       "      <th>roll_avg_fare_amount</th>\n",
       "      <th>date_rolled</th>\n",
       "      <th>daily_avg_fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datenum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1594598400000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.504167</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594684800000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.705000</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594771200000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.381250</td>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594857600000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594944000000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595030400000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595116800000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595203200000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595289600000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595376000000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595462400000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595548800000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595635200000000000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-07-25 22:50:11</td>\n",
       "      <td>2020-07-25 22:59:57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>16.350000</td>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595721600000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.350000</td>\n",
       "      <td>2020-07-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595808000000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.350000</td>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595894400000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595980800000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596067200000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2020-07-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596153600000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596240000000000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     VendorID tpep_pickup_datetime tpep_dropoff_datetime  \\\n",
       "datenum                                                                    \n",
       "1594598400000000000       NaN                  NaT                   NaT   \n",
       "1594684800000000000       NaN                  NaT                   NaT   \n",
       "1594771200000000000       NaN                  NaT                   NaT   \n",
       "1594857600000000000       NaN                  NaT                   NaT   \n",
       "1594944000000000000       NaN                  NaT                   NaT   \n",
       "1595030400000000000       NaN                  NaT                   NaT   \n",
       "1595116800000000000       NaN                  NaT                   NaT   \n",
       "1595203200000000000       NaN                  NaT                   NaT   \n",
       "1595289600000000000       NaN                  NaT                   NaT   \n",
       "1595376000000000000       NaN                  NaT                   NaT   \n",
       "1595462400000000000       NaN                  NaT                   NaT   \n",
       "1595548800000000000       NaN                  NaT                   NaT   \n",
       "1595635200000000000       2.0  2020-07-25 22:50:11   2020-07-25 22:59:57   \n",
       "1595721600000000000       NaN                  NaT                   NaT   \n",
       "1595808000000000000       NaN                  NaT                   NaT   \n",
       "1595894400000000000       NaN                  NaT                   NaT   \n",
       "1595980800000000000       NaN                  NaT                   NaT   \n",
       "1596067200000000000       NaN                  NaT                   NaT   \n",
       "1596153600000000000       NaN                  NaT                   NaT   \n",
       "1596240000000000000       NaN                  NaT                   NaT   \n",
       "\n",
       "                     passenger_count  trip_distance  tip_amount  fare_amount  \\\n",
       "datenum                                                                        \n",
       "1594598400000000000              NaN            NaN         NaN          NaN   \n",
       "1594684800000000000              NaN            NaN         NaN          NaN   \n",
       "1594771200000000000              NaN            NaN         NaN          NaN   \n",
       "1594857600000000000              NaN            NaN         NaN          NaN   \n",
       "1594944000000000000              NaN            NaN         NaN          NaN   \n",
       "1595030400000000000              NaN            NaN         NaN          NaN   \n",
       "1595116800000000000              NaN            NaN         NaN          NaN   \n",
       "1595203200000000000              NaN            NaN         NaN          NaN   \n",
       "1595289600000000000              NaN            NaN         NaN          NaN   \n",
       "1595376000000000000              NaN            NaN         NaN          NaN   \n",
       "1595462400000000000              NaN            NaN         NaN          NaN   \n",
       "1595548800000000000              NaN            NaN         NaN          NaN   \n",
       "1595635200000000000              1.0           2.08         0.0          9.0   \n",
       "1595721600000000000              NaN            NaN         NaN          NaN   \n",
       "1595808000000000000              NaN            NaN         NaN          NaN   \n",
       "1595894400000000000              NaN            NaN         NaN          NaN   \n",
       "1595980800000000000              NaN            NaN         NaN          NaN   \n",
       "1596067200000000000              NaN            NaN         NaN          NaN   \n",
       "1596153600000000000              NaN            NaN         NaN          NaN   \n",
       "1596240000000000000              NaN            NaN         NaN          NaN   \n",
       "\n",
       "                     total_amount    date_day  roll_avg_fare_amount  \\\n",
       "datenum                                                               \n",
       "1594598400000000000           NaN         NaN             20.504167   \n",
       "1594684800000000000           NaN         NaN             21.705000   \n",
       "1594771200000000000           NaN         NaN             16.381250   \n",
       "1594857600000000000           NaN         NaN             18.800000   \n",
       "1594944000000000000           NaN         NaN             18.800000   \n",
       "1595030400000000000           NaN         NaN             18.800000   \n",
       "1595116800000000000           NaN         NaN             18.800000   \n",
       "1595203200000000000           NaN         NaN             18.800000   \n",
       "1595289600000000000           NaN         NaN             18.800000   \n",
       "1595376000000000000           NaN         NaN             18.800000   \n",
       "1595462400000000000           NaN         NaN             18.800000   \n",
       "1595548800000000000           NaN         NaN             18.800000   \n",
       "1595635200000000000          12.8  2020-07-25             16.350000   \n",
       "1595721600000000000           NaN         NaN             16.350000   \n",
       "1595808000000000000           NaN         NaN             16.350000   \n",
       "1595894400000000000           NaN         NaN              6.800000   \n",
       "1595980800000000000           NaN         NaN              6.800000   \n",
       "1596067200000000000           NaN         NaN              6.800000   \n",
       "1596153600000000000           NaN         NaN              6.800000   \n",
       "1596240000000000000           NaN         NaN              6.800000   \n",
       "\n",
       "                    date_rolled  daily_avg_fare_amount  \n",
       "datenum                                                 \n",
       "1594598400000000000  2020-07-13                    NaN  \n",
       "1594684800000000000  2020-07-14                    NaN  \n",
       "1594771200000000000  2020-07-15                    NaN  \n",
       "1594857600000000000  2020-07-16                    NaN  \n",
       "1594944000000000000  2020-07-17                    NaN  \n",
       "1595030400000000000  2020-07-18                    NaN  \n",
       "1595116800000000000  2020-07-19                    NaN  \n",
       "1595203200000000000  2020-07-20                    NaN  \n",
       "1595289600000000000  2020-07-21                    NaN  \n",
       "1595376000000000000  2020-07-22                    NaN  \n",
       "1595462400000000000  2020-07-23                    NaN  \n",
       "1595548800000000000  2020-07-24                    NaN  \n",
       "1595635200000000000  2020-07-25                    9.0  \n",
       "1595721600000000000  2020-07-26                    NaN  \n",
       "1595808000000000000  2020-07-27                    NaN  \n",
       "1595894400000000000  2020-07-28                    NaN  \n",
       "1595980800000000000  2020-07-29                    NaN  \n",
       "1596067200000000000  2020-07-30                    NaN  \n",
       "1596153600000000000  2020-07-31                    NaN  \n",
       "1596240000000000000  2020-08-01                    NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_new[['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime','passenger_count','trip_distance','tip_amount',\n",
    "          'fare_amount', 'total_amount', 'date_day', 'roll_avg_fare_amount','date_rolled','daily_avg_fare_amount']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}